{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Web Mining Nama : Ahmad Sholiqin NIM : 180411100147 Pembimbing : @mulaab Refrensi : https://mulaab.github.io/ Material Crawling Data Preprocessing Data TF-IDF Modeling Data Library Toolkit : Tweepy SciPy Matplotlib NLTK Sastrawi Pandas Scikit-Learn Numpy","title":"Home"},{"location":"Modeling-Data/","text":"Modeling Data \u00b6 Pemodelan Data secara tradisional berkaitan dengan menganalisis interaksi pengguna dengan sistem dan dengan mengembangkan model kognitif yang membantu dalam desain antarmuka pengguna dan mekanisme interaksi. Elemen model pengguna dapat mencakup representasi tujuan, rencana, preferensi, tugas, dan/atau kemampuan tentang satu atau lebih jenis pengguna, klasifikasi pengguna ke dalam subkelompok atau stereotip, pembentukan asumsi tentang pengguna berdasarkan riwayat interaksi , dan generalisasi riwayat interaksi banyak pengguna ke dalam grup, di antara banyak lainnya. Predictive modeling, ada dua teknik yang dapat dilakukan dalam predictive modeling, yaitu: Classification Digunakan untuk membuat dugaan awal tentang class yang spesifik untuk setiap record dalam database dari satu setnilai class yang mungkin Value Prediction Digunakan untuk memperkirakan nilai numeric yang kontinu yang trasosiasi dengan record database. Teknik ini menggunakan teknik statistic klasik dari linier regression dan nonlinier regression. Cara Kerja Algoritma Dalam Algoritma KNN, K adalah jumlah tetangga terdekat, dimana jumlah tetangga dalah faktor penentu inti. K umumnya merupakan bilangan ganjil jika jumlah kelas adalah 2, dan bila K = 1 maka algoritma tersebut dikenal dengan nama algoritma tetangga terdekat. Algoritma K-NN memiliki tahaban sebagai berikut : Menghitung Jarak Menentukan tetangga terdekat Memberi suara untuk label Klasifikasi dengan Scikit-learn Pertama-tama mari kita muat TF-IDF.csv yang dibutuhkan dari dataset scikit-learn. import pandas as pd import numpy as np #Import scikit-learn dataset library from sklearn import datasets #Load dataset wine = pd . read_csv ( \"TF-IDF.csv\" ) wine . head () Memisahkan Data Untuk memahami kinerja model, membagi dataset menjadi training set dan test set adalah strategi yang baik. Mari kita pisahkan dataset dengan menggunakan fungsi train_test_split(). Anda harus melewati 3 parameter fitur, target, dan ukuran test_set. Selain itu, Anda dapat menggunakan random_state untuk memilih catatan secara acak. # Import train_test_split function from sklearn.model_selection import train_test_split # Split dataset into training set and test set X_train , X_test , y_train , y_test = train_test_split ( wine . data , wine . target , test_size = 0.3 ) # 70% training and 30% test Model Untuk K = 5 Untuk permodelan K = 5, dalam pengimplemntasian dalam bahasa python dapat dilakukan sebagai berikut. #Import knearest neighbors Classifier model from sklearn.neighbors import KNeighborsClassifier #Create KNN Classifier knn = KNeighborsClassifier ( n_neighbors = 5 ) #Train the model using the training sets knn . fit ( X_train , y_train ) #Predict the response for test dataset y_pred = knn . predict ( X_test ) Evaluasi Model untuk K = 5 Akurasi dalam permodelan dapat dihitung dengan membandingkan nilai set pengujian aktual dan nilai prediksi. Dalam implementasian pada Python sebagai berikut. #Import scikit-learn metrics module for accuracy calculation from sklearn import metrics # Model Accuracy, how often is the classifier correct? print ( \"Accuracy:\" , metrics . accuracy_score ( y_test , y_pred ))","title":"Modeling Data"},{"location":"Modeling-Data/#modeling_data","text":"Pemodelan Data secara tradisional berkaitan dengan menganalisis interaksi pengguna dengan sistem dan dengan mengembangkan model kognitif yang membantu dalam desain antarmuka pengguna dan mekanisme interaksi. Elemen model pengguna dapat mencakup representasi tujuan, rencana, preferensi, tugas, dan/atau kemampuan tentang satu atau lebih jenis pengguna, klasifikasi pengguna ke dalam subkelompok atau stereotip, pembentukan asumsi tentang pengguna berdasarkan riwayat interaksi , dan generalisasi riwayat interaksi banyak pengguna ke dalam grup, di antara banyak lainnya. Predictive modeling, ada dua teknik yang dapat dilakukan dalam predictive modeling, yaitu: Classification Digunakan untuk membuat dugaan awal tentang class yang spesifik untuk setiap record dalam database dari satu setnilai class yang mungkin Value Prediction Digunakan untuk memperkirakan nilai numeric yang kontinu yang trasosiasi dengan record database. Teknik ini menggunakan teknik statistic klasik dari linier regression dan nonlinier regression. Cara Kerja Algoritma Dalam Algoritma KNN, K adalah jumlah tetangga terdekat, dimana jumlah tetangga dalah faktor penentu inti. K umumnya merupakan bilangan ganjil jika jumlah kelas adalah 2, dan bila K = 1 maka algoritma tersebut dikenal dengan nama algoritma tetangga terdekat. Algoritma K-NN memiliki tahaban sebagai berikut : Menghitung Jarak Menentukan tetangga terdekat Memberi suara untuk label Klasifikasi dengan Scikit-learn Pertama-tama mari kita muat TF-IDF.csv yang dibutuhkan dari dataset scikit-learn. import pandas as pd import numpy as np #Import scikit-learn dataset library from sklearn import datasets #Load dataset wine = pd . read_csv ( \"TF-IDF.csv\" ) wine . head () Memisahkan Data Untuk memahami kinerja model, membagi dataset menjadi training set dan test set adalah strategi yang baik. Mari kita pisahkan dataset dengan menggunakan fungsi train_test_split(). Anda harus melewati 3 parameter fitur, target, dan ukuran test_set. Selain itu, Anda dapat menggunakan random_state untuk memilih catatan secara acak. # Import train_test_split function from sklearn.model_selection import train_test_split # Split dataset into training set and test set X_train , X_test , y_train , y_test = train_test_split ( wine . data , wine . target , test_size = 0.3 ) # 70% training and 30% test Model Untuk K = 5 Untuk permodelan K = 5, dalam pengimplemntasian dalam bahasa python dapat dilakukan sebagai berikut. #Import knearest neighbors Classifier model from sklearn.neighbors import KNeighborsClassifier #Create KNN Classifier knn = KNeighborsClassifier ( n_neighbors = 5 ) #Train the model using the training sets knn . fit ( X_train , y_train ) #Predict the response for test dataset y_pred = knn . predict ( X_test ) Evaluasi Model untuk K = 5 Akurasi dalam permodelan dapat dihitung dengan membandingkan nilai set pengujian aktual dan nilai prediksi. Dalam implementasian pada Python sebagai berikut. #Import scikit-learn metrics module for accuracy calculation from sklearn import metrics # Model Accuracy, how often is the classifier correct? print ( \"Accuracy:\" , metrics . accuracy_score ( y_test , y_pred ))","title":"Modeling Data"},{"location":"Preprocessing-Data/","text":"Preprocessing Data \u00b6 Pengertian Preprocessing merupakan salah satu tahapan yang penting untuk data pada proses mining. Data yang digunakan dalam proses mining tidak selamanya dalam kondisi yang ideal untuk diproses. Terkadang pada data tersebut terdapat berbagai permasalahan yang dapat menggangu hasil dari proses mining itu sendiri seperi diantaranya adalah missing value, data redundant, outliers, ataupun format data yang tidak sesuai dengan sistem. Oleh karenanya untuk mengatasi permasalahan tersebut dibutuhkan tahap Preprocessing. Preprocessing merupakan salah satu tahapan menghilangkan permasalahan-permasalahan yang dapat mengganggu hasil daripada proses data. Dalam kasus klasifikasi dokumen yang menggunakan data bertipe teks, terdapat beberapa macam proses yang dilakukan umumnya diantaranya case folding, filtering(remove punctution), stopword removal, stemming, tokenization dan sebagainya. Langkah-Langkah Proses Preprocessing Data Dokumen untuk diproses Representasi Data Cleaning data dengan menghilangkan tanda baca atau karakter selain teks dengan fungsi punctuation removal. Punctuation Removal Case Folding yang merupakan proses untuk merubah setiap kata menjadi sama, misal huruf kecil dengan menggunakan fungsi lowercase. Case Fold Stopword Removal, menghapus kata-kata yang terlalu umum dan kurang penting, ciri-ciri pada kata ini adalah frekwensi kemunculannya yang jumlahnya cukup banyak dibandingkan dengan kata yang lainnya, contoh kata : aku, kamu, dengan, yang dst. Stopword Removal Stemming, adalah proses untuk mengubah kata pada setiap kalimat ke bentuk dasar atau menghapus kata-kata imbuhan. Stemming Tokenizing. Merupakan tahap untuk memengal setiap kata dalam kalimat termasuk karakter. Tokenizing Nah, itulah contoh dari proses Preprocessing text, berikutnya data hasil Preprocessing dapat digunakan untuk keperluan proses Fitur seleksi, Fitur ekstraksi, Klasifikasi dan yang lainnya. Program Python Preprocessing Program dibuat secara manual programming, teman-teman dapat menggunakan cara lain menggunakan library yang mungkin sudah banyak tersedia, pemilihan cara pengerjaan tergantung dari studi kasus algoritma program yang dikerjakan. Buat File function.py copy code berikut ```python import re #regular expression import xlwt #library untuk membaca data pada excel from nltk.tokenize import word_tokenize #libray untuk tokenizing #Sastrawi Library untuk Stemming dan Stopword Removal Data Set Bahasa Indonesia from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory factori = StemmerFactory() stemmer = factori.create_stemmer() factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() class hadisClass(object): def init (self,hadis,k1,k2,k3): self.hadis = hadis self.k1 = k1 self.k2 = k2 self.k3 = k3 def openFile(wb): hadisContent = [] items= [] for sheet in wb.sheets(): num_row, num_col = sheet.nrows,4 for row in range(num_row): values = [] for col in range(num_col): if col == 0: x = (sheet.cell(row,col).value) value = (sheet.cell(row,col).value) values.append(value) hadisContent.append(x) item = hadisClass(*values) items.append(item) return hadisContent, items def prepro(datahadis): dhadis=[] for i in datahadis: cleanning = re.sub('[^a-zA-z\\s]','', i) casefold = cleanning.lower(); stopW = stopword.remove(casefold) stemming = stemmer.stem(stopW) tokens = word_tokenize(stemming) dhadis.append(tokens) return dhadis ``` Masih dalam folder yang sama buat File testRun.py copy code berikut import function as fc from xlrd import open_workbook wb = open_workbook ( 'book1.xlsx' ) data , items = fc . openFile ( wb ) datatest = [] for j in data : datatest . append ( j ) datatest = fc . prepro ( datatest ) print ( datatest ) Jika berhasil bentuk output data akan seperti ini","title":"Preprocessing Data"},{"location":"Preprocessing-Data/#preprocessing_data","text":"Pengertian Preprocessing merupakan salah satu tahapan yang penting untuk data pada proses mining. Data yang digunakan dalam proses mining tidak selamanya dalam kondisi yang ideal untuk diproses. Terkadang pada data tersebut terdapat berbagai permasalahan yang dapat menggangu hasil dari proses mining itu sendiri seperi diantaranya adalah missing value, data redundant, outliers, ataupun format data yang tidak sesuai dengan sistem. Oleh karenanya untuk mengatasi permasalahan tersebut dibutuhkan tahap Preprocessing. Preprocessing merupakan salah satu tahapan menghilangkan permasalahan-permasalahan yang dapat mengganggu hasil daripada proses data. Dalam kasus klasifikasi dokumen yang menggunakan data bertipe teks, terdapat beberapa macam proses yang dilakukan umumnya diantaranya case folding, filtering(remove punctution), stopword removal, stemming, tokenization dan sebagainya. Langkah-Langkah Proses Preprocessing Data Dokumen untuk diproses Representasi Data Cleaning data dengan menghilangkan tanda baca atau karakter selain teks dengan fungsi punctuation removal. Punctuation Removal Case Folding yang merupakan proses untuk merubah setiap kata menjadi sama, misal huruf kecil dengan menggunakan fungsi lowercase. Case Fold Stopword Removal, menghapus kata-kata yang terlalu umum dan kurang penting, ciri-ciri pada kata ini adalah frekwensi kemunculannya yang jumlahnya cukup banyak dibandingkan dengan kata yang lainnya, contoh kata : aku, kamu, dengan, yang dst. Stopword Removal Stemming, adalah proses untuk mengubah kata pada setiap kalimat ke bentuk dasar atau menghapus kata-kata imbuhan. Stemming Tokenizing. Merupakan tahap untuk memengal setiap kata dalam kalimat termasuk karakter. Tokenizing Nah, itulah contoh dari proses Preprocessing text, berikutnya data hasil Preprocessing dapat digunakan untuk keperluan proses Fitur seleksi, Fitur ekstraksi, Klasifikasi dan yang lainnya. Program Python Preprocessing Program dibuat secara manual programming, teman-teman dapat menggunakan cara lain menggunakan library yang mungkin sudah banyak tersedia, pemilihan cara pengerjaan tergantung dari studi kasus algoritma program yang dikerjakan. Buat File function.py copy code berikut ```python import re #regular expression import xlwt #library untuk membaca data pada excel from nltk.tokenize import word_tokenize #libray untuk tokenizing #Sastrawi Library untuk Stemming dan Stopword Removal Data Set Bahasa Indonesia from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory factori = StemmerFactory() stemmer = factori.create_stemmer() factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() class hadisClass(object): def init (self,hadis,k1,k2,k3): self.hadis = hadis self.k1 = k1 self.k2 = k2 self.k3 = k3 def openFile(wb): hadisContent = [] items= [] for sheet in wb.sheets(): num_row, num_col = sheet.nrows,4 for row in range(num_row): values = [] for col in range(num_col): if col == 0: x = (sheet.cell(row,col).value) value = (sheet.cell(row,col).value) values.append(value) hadisContent.append(x) item = hadisClass(*values) items.append(item) return hadisContent, items def prepro(datahadis): dhadis=[] for i in datahadis: cleanning = re.sub('[^a-zA-z\\s]','', i) casefold = cleanning.lower(); stopW = stopword.remove(casefold) stemming = stemmer.stem(stopW) tokens = word_tokenize(stemming) dhadis.append(tokens) return dhadis ``` Masih dalam folder yang sama buat File testRun.py copy code berikut import function as fc from xlrd import open_workbook wb = open_workbook ( 'book1.xlsx' ) data , items = fc . openFile ( wb ) datatest = [] for j in data : datatest . append ( j ) datatest = fc . prepro ( datatest ) print ( datatest ) Jika berhasil bentuk output data akan seperti ini","title":"Preprocessing Data"},{"location":"TF-IDF/","text":"TF-IDF (Term Frequency-Inverse Document Frequency) \u00b6 Algoritma TF-IDF (Term Frequency \u2013 Inverse Document Frequency) adalah salah satu algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase/kalimat dengan sekumpulan dokumen. Contoh yang dibahas kali ini adalah mengenai penentuan urutan peringkat data berdasarkan query yang digunakan. Inti utama dari algoritma ini adalah melakukan perhitungan nilai TF dan nilai IDF dari sebuah setiap kata kunci terhadap masing-masing dokumen. Nilai TF dihitung dengan rumus TF = jumlah frekuensi kata terpilih / jumlah kata dan nilai IDF dihitung dengan rumus IDF = log(jumlah dokumen / jumlah frekuensi kata terpilih) . Selanjutnya adalah melakukan perkalian antara nilai TF dan IDF untuk mendapatkan jawaban akhir. Diasumsikan data kalimat yang tersedia adalah sebagai berikut: isi kalimat Saya suka suami situ sebab suami situ suka senyum-seyum sama saya. Santapan kita setiap jam setengah satu siang satu soto sapi sama seratus tusuk sate sapi pula. Saya sebal sama situ sebab situ suka senyum-senyum sama suami saya sehingga suami saya suka senyum-senyum sendiri saja. Sempat-sempatnya semut-semut itu saling senyum-senyum dan salam-salaman sama semut-semut yang mau senyum-senyum dan salam-salaman sama semut-semut itu. Contoh data awal adalah sebagai berikut: daftarDokumen As New List ( Of String ) daftarDokumen . Add ( \"Saya suka sama suami situ sebab suami situ suka senyum-senyum sama saya.\" ) daftarDokumen . Add ( \"Santapan kita setiap jam setengah satu siang satu soto sapi sama seratus tusuk sate sapi pula.\" ) daftarDokumen . Add ( \"Saya sebal sama situ sebab situ suka senyum-senyum sama suami saya sehingga suami saya suka senyum-senyum sendiri saja.\" ) daftarDokumen . Add ( \"Sempat-sempatnya semut-semut itu saling senyum-senyum dan salam-salaman sama semut-semut yang mau senyum-senyum dan salam-salaman sama semut-semut itu.\" ) Dan query data yang digunakan adalah isi kalimat Sapi saling suka Contoh data baru adalah sebagai berikut: Dim query As String = \"Sapi saling suka\" Langkah-langkah penggunaan algoritma ini adalah Lakukan proses tokenizing dan lowercase pada masing-masing dokumen dan query Setiap kata akan dijadikan huruf kecil semua, dan kemudian dilakukan proses penghilangan tanda baca Penjelasan lebih detail tentang fungsi ini dapat dilihat pada penjelasan skrip dibawah ini. For i As Integer = 0 To daftarDokumen . Count - 1 daftarDokumen ( i ) = Tokenizing ( daftarDokumen ( i ) . ToLower ) Next query = Tokenizing ( query . ToLower ) Gunakan fungsi ini untuk menghilangkan tanda baca Tanda baca yang diperhitungkan adalah: titik ,koma, titik koma, titik dua, hubung -, tanda tanya, tanda seru, kurung biasa (), kurung kotak [], kurung kurawal {}, tanda petik satu, tanda petik ganda, garis miring Public Function Tokenizing ( ByVal input As String ) As String input = input . Replace ( \".\" , \"\" ) input = input . Replace ( \",\" , \"\" ) input = input . Replace ( \":\" , \"\" ) input = input . Replace ( \"-\" , \" \" ) input = input . Replace ( \"?\" , \"\" ) input = input . Replace ( \"!\" , \"\" ) input = input . Replace ( \"(\" , \"\" ) input = input . Replace ( \")\" , \"\" ) input = input . Replace ( \"[\" , \"\" ) input = input . Replace ( \"]\" , \"\" ) input = input . Replace ( \"{\" , \"\" ) input = input . Replace ( \"}\" , \"\" ) input = input . Replace ( \"'\" , \"\" ) input = input . Replace ( \"\"\"\", \"\") input = input.Replace(\"/\", \"\") Return input End Function Hitung jumlah dari masing-masing kata yang terdapat pada masing-masing dokumen Sebagai contoh jika dokumen berisi kalimat \u201chati hati ya\u201d Maka jumlah kata \u201chati\u201d adalah 2, dan jumlah kata \u201cya\u201d adalah 1 jumlahKataPerDokumen ( i ) = New List ( Of JumlahKata ) Dim daftarKata As String () = daftarDokumen ( i ) . Split ( \" \" ) Dim cekKata As New List ( Of String ) For j As Integer = 0 To daftarKata . Count - 1 If Not cekKata . Contains ( daftarKata ( j )) Then Dim jumlah As Integer = 0 For k As Integer = 0 To daftarKata . Count - 1 If daftarKata ( j ) = daftarKata ( k ) Then jumlah += 1 Next jumlahKataPerDokumen ( i ) . Add ( New JumlahKata ( daftarKata ( j ), jumlah )) cekKata . Add ( daftarKata ( j )) End If Next Hitung nilai TF dengan rumus TF = jumlah frekuensi kata terpilih / jumlah kata For j As Integer = 0 To daftarKataQuery . Count - 1 Dim frekuensi As Integer = 0 For k As Integer = 0 To jumlahKataPerDokumen ( i ) . Count - 1 If daftarKataQuery ( j ) = jumlahKataPerDokumen ( i )( k ) . kata Then frekuensi = jumlahKataPerDokumen ( i )( k ) . jumlah Exit For End If Next TF ( i ) . Add ( frekuensi / daftarKata . Length ) Next Hitung IDF dengan rumus IDF = log(jumlah dokumen / jumlah frekuensi kata terpilih) Dim frekuensiDokumen ( daftarKataQuery . Count - 1 ) As Integer For i As Integer = 0 To daftarDokumen . Count - 1 For j As Integer = 0 To daftarKataQuery . Count - 1 If TF ( i )( j ) > 0 Then frekuensiDokumen ( j ) += 1 Next Next For j As Integer = 0 To daftarKataQuery . Count - 1 IDF ( j ) = Math . Log10 ( daftarDokumen . Count / frekuensiDokumen ( j )) Next Nilai jawaban dilakukan dengan cara melakukan perkalian antara nilai TF dan nilai IDF Tampilkan hasil jawaban akhir pada layar For i As Integer = 0 To daftarDokumen . Count - 1 For j As Integer = 0 To daftarKataQuery . Count - 1 Console . WriteLine ( \"Nilai TF-IDF untuk kata \" & daftarKataQuery ( j ) . PadRight ( 8 ) & \" terhadap dokumen \" & ( i + 1 ) & \" adalah \" & ( TF ( i )( j ) * IDF ( j )) . ToString ( \"F4\" )) Next Next Hasil akhir adalah:","title":"TF-IDF Data"},{"location":"TF-IDF/#tf-idf_term_frequency-inverse_document_frequency","text":"Algoritma TF-IDF (Term Frequency \u2013 Inverse Document Frequency) adalah salah satu algoritma yang dapat digunakan untuk menganalisa hubungan antara sebuah frase/kalimat dengan sekumpulan dokumen. Contoh yang dibahas kali ini adalah mengenai penentuan urutan peringkat data berdasarkan query yang digunakan. Inti utama dari algoritma ini adalah melakukan perhitungan nilai TF dan nilai IDF dari sebuah setiap kata kunci terhadap masing-masing dokumen. Nilai TF dihitung dengan rumus TF = jumlah frekuensi kata terpilih / jumlah kata dan nilai IDF dihitung dengan rumus IDF = log(jumlah dokumen / jumlah frekuensi kata terpilih) . Selanjutnya adalah melakukan perkalian antara nilai TF dan IDF untuk mendapatkan jawaban akhir. Diasumsikan data kalimat yang tersedia adalah sebagai berikut: isi kalimat Saya suka suami situ sebab suami situ suka senyum-seyum sama saya. Santapan kita setiap jam setengah satu siang satu soto sapi sama seratus tusuk sate sapi pula. Saya sebal sama situ sebab situ suka senyum-senyum sama suami saya sehingga suami saya suka senyum-senyum sendiri saja. Sempat-sempatnya semut-semut itu saling senyum-senyum dan salam-salaman sama semut-semut yang mau senyum-senyum dan salam-salaman sama semut-semut itu. Contoh data awal adalah sebagai berikut: daftarDokumen As New List ( Of String ) daftarDokumen . Add ( \"Saya suka sama suami situ sebab suami situ suka senyum-senyum sama saya.\" ) daftarDokumen . Add ( \"Santapan kita setiap jam setengah satu siang satu soto sapi sama seratus tusuk sate sapi pula.\" ) daftarDokumen . Add ( \"Saya sebal sama situ sebab situ suka senyum-senyum sama suami saya sehingga suami saya suka senyum-senyum sendiri saja.\" ) daftarDokumen . Add ( \"Sempat-sempatnya semut-semut itu saling senyum-senyum dan salam-salaman sama semut-semut yang mau senyum-senyum dan salam-salaman sama semut-semut itu.\" ) Dan query data yang digunakan adalah isi kalimat Sapi saling suka Contoh data baru adalah sebagai berikut: Dim query As String = \"Sapi saling suka\" Langkah-langkah penggunaan algoritma ini adalah Lakukan proses tokenizing dan lowercase pada masing-masing dokumen dan query Setiap kata akan dijadikan huruf kecil semua, dan kemudian dilakukan proses penghilangan tanda baca Penjelasan lebih detail tentang fungsi ini dapat dilihat pada penjelasan skrip dibawah ini. For i As Integer = 0 To daftarDokumen . Count - 1 daftarDokumen ( i ) = Tokenizing ( daftarDokumen ( i ) . ToLower ) Next query = Tokenizing ( query . ToLower ) Gunakan fungsi ini untuk menghilangkan tanda baca Tanda baca yang diperhitungkan adalah: titik ,koma, titik koma, titik dua, hubung -, tanda tanya, tanda seru, kurung biasa (), kurung kotak [], kurung kurawal {}, tanda petik satu, tanda petik ganda, garis miring Public Function Tokenizing ( ByVal input As String ) As String input = input . Replace ( \".\" , \"\" ) input = input . Replace ( \",\" , \"\" ) input = input . Replace ( \":\" , \"\" ) input = input . Replace ( \"-\" , \" \" ) input = input . Replace ( \"?\" , \"\" ) input = input . Replace ( \"!\" , \"\" ) input = input . Replace ( \"(\" , \"\" ) input = input . Replace ( \")\" , \"\" ) input = input . Replace ( \"[\" , \"\" ) input = input . Replace ( \"]\" , \"\" ) input = input . Replace ( \"{\" , \"\" ) input = input . Replace ( \"}\" , \"\" ) input = input . Replace ( \"'\" , \"\" ) input = input . Replace ( \"\"\"\", \"\") input = input.Replace(\"/\", \"\") Return input End Function Hitung jumlah dari masing-masing kata yang terdapat pada masing-masing dokumen Sebagai contoh jika dokumen berisi kalimat \u201chati hati ya\u201d Maka jumlah kata \u201chati\u201d adalah 2, dan jumlah kata \u201cya\u201d adalah 1 jumlahKataPerDokumen ( i ) = New List ( Of JumlahKata ) Dim daftarKata As String () = daftarDokumen ( i ) . Split ( \" \" ) Dim cekKata As New List ( Of String ) For j As Integer = 0 To daftarKata . Count - 1 If Not cekKata . Contains ( daftarKata ( j )) Then Dim jumlah As Integer = 0 For k As Integer = 0 To daftarKata . Count - 1 If daftarKata ( j ) = daftarKata ( k ) Then jumlah += 1 Next jumlahKataPerDokumen ( i ) . Add ( New JumlahKata ( daftarKata ( j ), jumlah )) cekKata . Add ( daftarKata ( j )) End If Next Hitung nilai TF dengan rumus TF = jumlah frekuensi kata terpilih / jumlah kata For j As Integer = 0 To daftarKataQuery . Count - 1 Dim frekuensi As Integer = 0 For k As Integer = 0 To jumlahKataPerDokumen ( i ) . Count - 1 If daftarKataQuery ( j ) = jumlahKataPerDokumen ( i )( k ) . kata Then frekuensi = jumlahKataPerDokumen ( i )( k ) . jumlah Exit For End If Next TF ( i ) . Add ( frekuensi / daftarKata . Length ) Next Hitung IDF dengan rumus IDF = log(jumlah dokumen / jumlah frekuensi kata terpilih) Dim frekuensiDokumen ( daftarKataQuery . Count - 1 ) As Integer For i As Integer = 0 To daftarDokumen . Count - 1 For j As Integer = 0 To daftarKataQuery . Count - 1 If TF ( i )( j ) > 0 Then frekuensiDokumen ( j ) += 1 Next Next For j As Integer = 0 To daftarKataQuery . Count - 1 IDF ( j ) = Math . Log10 ( daftarDokumen . Count / frekuensiDokumen ( j )) Next Nilai jawaban dilakukan dengan cara melakukan perkalian antara nilai TF dan nilai IDF Tampilkan hasil jawaban akhir pada layar For i As Integer = 0 To daftarDokumen . Count - 1 For j As Integer = 0 To daftarKataQuery . Count - 1 Console . WriteLine ( \"Nilai TF-IDF untuk kata \" & daftarKataQuery ( j ) . PadRight ( 8 ) & \" terhadap dokumen \" & ( i + 1 ) & \" adalah \" & ( TF ( i )( j ) * IDF ( j )) . ToString ( \"F4\" )) Next Next Hasil akhir adalah:","title":"TF-IDF (Term Frequency-Inverse Document Frequency)"},{"location":"crawl-data/","text":"Crawling Data Menggunakan Python Untuk crawling data twitter bisa menggunakan library Tweepy . Untuk menginstall Tweepy nya ada 3 cara : Installation \u00b6 Menggunakan pip direkomendasikan Tweepy dapat diinstall dengan menggunakan pip : pip install tweepy Dengan clone repository github Tweepy dapat diinstal dengan menggunakan clone repository dari github : git clone https : // github . com / tweepy / tweepy . git cd tweepy pip install Dengan Install langsung dari repository github Tweepy dapat juga diinstal tanpa harus meng-clone repository dari github dengan cara : pip install git+https://github.com/tweepy/tweepy.git BeautifulSoup \u00b6 BeautifulSoup merupakan library python digunakan untuk melakukan web scraping dalam package bs4. untuk install bs4 gunakan pip atau conda pip install bs4 Setelah sukses terinstall cara memanggil library BeautifulSoup adalah sebagai berikut from bs4 import BeautifulSoup contoh penggunaan BeautifulSoup ''' <html> <head> <title>Tutorial BeautifulSoup</title> </head> <body> <p class=\"judul\">Judul Dokumen</p> <p class=\"paragraf\">Ini adalah contoh paragraf</p> <a href=\"https://ngoding.com\" class=\"url\">Ngoding</a> </body> </html> ''' Kemudian gunakan library BeautifulSoup untuk mengekstrak kode HTML from bs4 import BeautifulSoup html_soup = BeautifulSoup ( dokumen , 'html.parser' ) print ( html_soup ) Fungsi find() Untuk mengambil potongan kode HTML atau konten dari HTML gunakan fungsi find() Fungsi find() akan mengambil data berdasarkan tag HTML. Jika terdapat tag HTML yang sama lebih dari satu maka yang diambil adalah tag yang paling atas di halaman HTML judul = html_soup . find ( 'p' ) print ( judul ) Jika ingin mengambil nilai dari tag <p> dengan nama class tertentu maka tambahkan parameter class_ di dalam fungsi find() Misal kita tambahkan parameter class judul dan paragraf untuk mengambil nilai dari tag <p> dengan kedua class tersebut judul = html_soup . find ( 'p' , class_ = 'judul' ) paragraf = html_soup . find ( 'p' , class_ = 'paragraf' ) print ( judul ) print ( paragraf ) untuk mengambil hanya konten dari HTML (tanpa kode HTMLnya) tambahkan sintaks text diakhir statement judul_saja = html_soup . find ( 'p' , class_ = 'judul' ) . text print ( judul_saja ) Fungsi find_all() Fungsi find() hanya dapat mengekstrak satu ouput sedangkan biasanya banyak tag HTML yang sama yang ingin diambil semuanya. Untuk mengambil konten HTML dengan tag yang sama gunakan fungsi find_all() all_paragraf = html_soup . find_all ( 'p' ) print ( all_paragraf ) Biasanya fungsi find_all() digunakan untuk mengambil data yang berbentuk table atau list Sebenarnya masih banyak fungsi lain dari library BeautifulSoup tetapi fungsi find() dan find_all() paling banyak digunakan untuk ekstrak data dari website. Berikut kode lengkap dari case BeautifulSoup diatas from bs4 import BeautifulSoup dokumen = ''' <html> <head> <title>Tutorial BeautifulSoup</title> </head> <body> <p class=\"judul\">Judul Dokumen</p> <p class=\"paragraf\">Ini adalah contoh paragraf</p> <a href=\"https://ngoding.com\" class=\"url\">Ngoding</a> </body> </html> ''' html_soup = BeautifulSoup ( dokumen , 'html.parser' ) judul = html_soup . find ( 'p' , class_ = 'judul' ) paragraf = html_soup . find ( 'p' , class_ = 'paragraf' ) judul_saja = html_soup . find ( 'p' , class_ = 'judul' ) . text print ( judul ) print ( paragraf ) print ( judul_saja ) all_paragraf = html_soup . find_all ( 'p' ) print ( all_paragraf )","title":"Crawling Data"},{"location":"crawl-data/#installation","text":"Menggunakan pip direkomendasikan Tweepy dapat diinstall dengan menggunakan pip : pip install tweepy Dengan clone repository github Tweepy dapat diinstal dengan menggunakan clone repository dari github : git clone https : // github . com / tweepy / tweepy . git cd tweepy pip install Dengan Install langsung dari repository github Tweepy dapat juga diinstal tanpa harus meng-clone repository dari github dengan cara : pip install git+https://github.com/tweepy/tweepy.git","title":"Installation"},{"location":"crawl-data/#beautifulsoup","text":"BeautifulSoup merupakan library python digunakan untuk melakukan web scraping dalam package bs4. untuk install bs4 gunakan pip atau conda pip install bs4 Setelah sukses terinstall cara memanggil library BeautifulSoup adalah sebagai berikut from bs4 import BeautifulSoup contoh penggunaan BeautifulSoup ''' <html> <head> <title>Tutorial BeautifulSoup</title> </head> <body> <p class=\"judul\">Judul Dokumen</p> <p class=\"paragraf\">Ini adalah contoh paragraf</p> <a href=\"https://ngoding.com\" class=\"url\">Ngoding</a> </body> </html> ''' Kemudian gunakan library BeautifulSoup untuk mengekstrak kode HTML from bs4 import BeautifulSoup html_soup = BeautifulSoup ( dokumen , 'html.parser' ) print ( html_soup ) Fungsi find() Untuk mengambil potongan kode HTML atau konten dari HTML gunakan fungsi find() Fungsi find() akan mengambil data berdasarkan tag HTML. Jika terdapat tag HTML yang sama lebih dari satu maka yang diambil adalah tag yang paling atas di halaman HTML judul = html_soup . find ( 'p' ) print ( judul ) Jika ingin mengambil nilai dari tag <p> dengan nama class tertentu maka tambahkan parameter class_ di dalam fungsi find() Misal kita tambahkan parameter class judul dan paragraf untuk mengambil nilai dari tag <p> dengan kedua class tersebut judul = html_soup . find ( 'p' , class_ = 'judul' ) paragraf = html_soup . find ( 'p' , class_ = 'paragraf' ) print ( judul ) print ( paragraf ) untuk mengambil hanya konten dari HTML (tanpa kode HTMLnya) tambahkan sintaks text diakhir statement judul_saja = html_soup . find ( 'p' , class_ = 'judul' ) . text print ( judul_saja ) Fungsi find_all() Fungsi find() hanya dapat mengekstrak satu ouput sedangkan biasanya banyak tag HTML yang sama yang ingin diambil semuanya. Untuk mengambil konten HTML dengan tag yang sama gunakan fungsi find_all() all_paragraf = html_soup . find_all ( 'p' ) print ( all_paragraf ) Biasanya fungsi find_all() digunakan untuk mengambil data yang berbentuk table atau list Sebenarnya masih banyak fungsi lain dari library BeautifulSoup tetapi fungsi find() dan find_all() paling banyak digunakan untuk ekstrak data dari website. Berikut kode lengkap dari case BeautifulSoup diatas from bs4 import BeautifulSoup dokumen = ''' <html> <head> <title>Tutorial BeautifulSoup</title> </head> <body> <p class=\"judul\">Judul Dokumen</p> <p class=\"paragraf\">Ini adalah contoh paragraf</p> <a href=\"https://ngoding.com\" class=\"url\">Ngoding</a> </body> </html> ''' html_soup = BeautifulSoup ( dokumen , 'html.parser' ) judul = html_soup . find ( 'p' , class_ = 'judul' ) paragraf = html_soup . find ( 'p' , class_ = 'paragraf' ) judul_saja = html_soup . find ( 'p' , class_ = 'judul' ) . text print ( judul ) print ( paragraf ) print ( judul_saja ) all_paragraf = html_soup . find_all ( 'p' ) print ( all_paragraf )","title":"BeautifulSoup"},{"location":"license/","text":"License \u00b6 MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY Support Author \u00b6 Amazon wish list","title":"License"},{"location":"license/#license","text":"MIT License The graduate cap icon made by Freepik from www.flaticon.com is licensed by CC 3.0 BY","title":"License"},{"location":"license/#support_author","text":"Amazon wish list","title":"Support Author"},{"location":"material-for-mkdocs/","text":"Material for MkDocs \u00b6 MkDocs \u00b6 mkdocs/mkdocs: Project documentation with Markdown - GitHub Material for MkDocs \u00b6 squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs","text":"","title":"Material for MkDocs"},{"location":"material-for-mkdocs/#mkdocs","text":"mkdocs/mkdocs: Project documentation with Markdown - GitHub","title":"MkDocs"},{"location":"material-for-mkdocs/#material_for_mkdocs_1","text":"squidfunk/mkdocs-material: A Material Design theme for MkDocs","title":"Material for MkDocs"},{"location":"extensions/code-hilite/","text":"CodeHilite \u00b6 CodeHilite - Material for MkDocs Supported languages - Pygments Configure mkdocs.yml \u00b6 markdown_extensions: - codehilite","title":"CodeHilite"},{"location":"extensions/code-hilite/#codehilite","text":"CodeHilite - Material for MkDocs Supported languages - Pygments","title":"CodeHilite"},{"location":"extensions/code-hilite/#configure_mkdocsyml","text":"markdown_extensions: - codehilite","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/","text":"Footnote \u00b6 Footnotes - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - footnotes Example \u00b6 Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Footnote"},{"location":"extensions/footnote/#footnote","text":"Footnotes - Material for MkDocs","title":"Footnote"},{"location":"extensions/footnote/#configure_mkdocsyml","text":"markdown_extensions: - footnotes","title":"Configure mkdocs.yml"},{"location":"extensions/footnote/#example","text":"Footnote example 1. 1 Footnote example 2. 2 One line \u21a9 First line Second line \u21a9","title":"Example"},{"location":"extensions/mathjax/","text":"MathJax \u00b6 PyMdown - Material for MkDocs Configure mkdocs.yml \u00b6 markdown_extensions: - mdx_math: enable_dollar_delimiter: True Example code \u00b6 $$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$ Example rendering \u00b6 P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"MathJax"},{"location":"extensions/mathjax/#mathjax","text":"PyMdown - Material for MkDocs","title":"MathJax"},{"location":"extensions/mathjax/#configure_mkdocsyml","text":"markdown_extensions: - mdx_math: enable_dollar_delimiter: True","title":"Configure mkdocs.yml"},{"location":"extensions/mathjax/#example_code","text":"$$ P \\c dot Q = \\| P \\|\\| Q \\|\\c os \\a lpha $$","title":"Example code"},{"location":"extensions/mathjax/#example_rendering","text":"P\\cdot Q = \\|P\\|\\|Q\\|\\cos\\alpha","title":"Example rendering"}]}